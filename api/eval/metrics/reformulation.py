"""
Query reformulation quality metrics using LLM-as-judge.

Evaluates whether reformulated queries are diverse, semantically aligned,
and well-formed for the NLP/CL domain.
"""

import asyncio
import json
import logging
from typing import Any, Dict, List

from langchain_groq import ChatGroq

from eval.config import eval_config

logger = logging.getLogger(__name__)

REFORMULATION_EVAL_PROMPT = """You are evaluating search query reformulations generated by a RAG system.

The system takes a user's original search query and generates multiple alternative search queries to improve retrieval recall.

Original query: {original_query}

Reformulated queries:
{reformulated_queries}

Evaluate each dimension from 0.0 to 1.0:

1. **Semantic Fidelity**: Do the reformulations preserve the original query's intent? They should search for the same topic.
2. **Diversity**: Do the reformulations capture different aspects, phrasings, or perspectives? They should NOT just be paraphrases of each other.
3. **Specificity**: Are the reformulations well-formed search queries? Not too vague, not too narrow.
4. **Domain Appropriateness**: Do they use appropriate NLP/computational linguistics terminology?

Return ONLY a JSON object:
{{"fidelity": <float>, "diversity": <float>, "specificity": <float>, "domain_appropriateness": <float>, "overall": <float>, "explanation": "<brief reason>"}}"""


def _get_judge_llm() -> ChatGroq:
    """Create a judge LLM instance."""
    return ChatGroq(
        model=eval_config.JUDGE_MODEL,
        temperature=eval_config.JUDGE_TEMPERATURE,
        max_tokens=eval_config.JUDGE_MAX_TOKENS,
    )


async def evaluate_reformulation_quality(
    original_query: str,
    reformulated_queries: List[str],
) -> Dict[str, Any]:
    """
    Evaluate the quality of reformulated search queries.

    Args:
        original_query: The user's original search query
        reformulated_queries: List of reformulated queries (may include original)

    Returns:
        Dict with fidelity, diversity, specificity, domain_appropriateness, overall scores
    """
    # Filter out the original query if it's included
    unique_reformulations = [
        q for q in reformulated_queries if q.lower().strip() != original_query.lower().strip()
    ]

    if not unique_reformulations:
        return {
            "fidelity": 0.0,
            "diversity": 0.0,
            "specificity": 0.0,
            "domain_appropriateness": 0.0,
            "overall": 0.0,
            "explanation": "No reformulations generated (only original query)",
        }

    formatted_queries = "\n".join(
        f"{i+1}. {q}" for i, q in enumerate(unique_reformulations)
    )

    prompt = REFORMULATION_EVAL_PROMPT.format(
        original_query=original_query,
        reformulated_queries=formatted_queries,
    )

    llm = _get_judge_llm()

    try:
        response = await llm.ainvoke(prompt)
        content = response.content.strip()

        if content.startswith("```"):
            content = content.split("\n", 1)[1]
            content = content.rsplit("```", 1)[0]
            content = content.strip()

        result = json.loads(content)
        return result
    except json.JSONDecodeError as e:
        logger.error(f"Failed to parse reformulation judge response: {e}")
        return {
            "fidelity": 0.0,
            "diversity": 0.0,
            "specificity": 0.0,
            "domain_appropriateness": 0.0,
            "overall": 0.0,
            "explanation": f"JSON parse error: {e}",
        }
    except Exception as e:
        logger.error(f"Reformulation judge call failed: {e}")
        return {
            "fidelity": 0.0,
            "diversity": 0.0,
            "specificity": 0.0,
            "domain_appropriateness": 0.0,
            "overall": 0.0,
            "explanation": f"Judge error: {e}",
        }
